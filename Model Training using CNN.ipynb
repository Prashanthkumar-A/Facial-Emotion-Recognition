{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#import the required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.layers import Activation, Convolution2D, Dropout, Conv2D,\n",
    "from keras.layers import AveragePooling2D, BatchNormalization\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import SeparableConv2D\n",
    "from keras import layers\n",
    "from keras.regularizers import l2\n",
    "import sys, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.layers import Dense\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# reading csv to dataframe\n",
    "df=pd.read_csv('fer2013.csv')\n",
    "\n",
    "# print(df.info())\n",
    "# print(df[\"Usage\"].value_counts())\n",
    "# print(df.head())\n",
    "\n",
    "\n",
    "\n",
    "x_train,y_train,x_test,y_test=[],[],[],[]\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    val=row['pixels'].split(\" \")\n",
    "    try:\n",
    "        if 'Training' in row['Usage']:\n",
    "            x_train.append(np.array(val,'float32'))\n",
    "            y_train.append(row['emotion'])\n",
    "        elif 'PublicTest' in row['Usage']:\n",
    "            x_test.append(np.array(val,'float32'))\n",
    "            y_test.append(row['emotion'])\n",
    "    except:\n",
    "        print(f\"error occured at index :{index} and row:{row}\")\n",
    "\n",
    "\n",
    "num_features = 64\n",
    "num_labels = 7\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "width, height = 48, 48\n",
    "\n",
    "\n",
    "x_train = np.array(x_train,'float32')\n",
    "y_train = np.array(y_train,'float32')\n",
    "x_test = np.array(x_test,'float32')\n",
    "y_test= np.array(y_test,'float32')\n",
    "\n",
    "y_train=np_utils.to_categorical(y_train, num_classes=num_labels)\n",
    "y_test=np_utils.to_categorical(y_test, num_classes=num_labels)\n",
    "\n",
    "\n",
    "#normalizing data between oand 1\n",
    "x_train -= np.mean(x_train, axis=0)\n",
    "x_train /= np.std(x_train, axis=0)\n",
    "\n",
    "x_test -= np.mean(x_test, axis=0)\n",
    "x_test /= np.std(x_test, axis=0)\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], 48, 48, 1)\n",
    "\n",
    "x_test = x_test.reshape(x_test.shape[0], 48, 48, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/angara/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/angara/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/angara/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/angara/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/angara/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/angara/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/angara/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/angara/anaconda3/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
     ]
    }
   ],
   "source": [
    "#1st convolution layer\n",
    "model = Sequential()\n",
    "l2_regularization=0.01\n",
    "regularization = l2(l2_regularization)\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=(5, 5), activation='relu',kernel_regularizer=regularization, input_shape=(x_train.shape[1:]),padding='same'))\n",
    "model.add(Conv2D(64,kernel_size= (5,5), activation='relu',kernel_regularizer=regularization,padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "#2nd convolution layer\n",
    "model.add(Conv2D(64, (5,5), activation='relu',kernel_regularizer=regularization,padding='same'))\n",
    "model.add(Conv2D(64, (5,5), activation='relu',kernel_regularizer=regularization,padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "#3rd convolution layer\n",
    "model.add(Conv2D(128, (3, 3), activation='relu',kernel_regularizer=regularization,padding='same'))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu',kernel_regularizer=regularization,padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(7))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/angara/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#Compliling the model\n",
    "model.compile(loss=categorical_crossentropy,\n",
    "              optimizer=Adam(1e-4,beta_1=0.95),\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the model at each epoch\n",
    "from keras.callbacks import CSVLogger, ModelCheckpoint, EarlyStopping\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "patience=50\n",
    "base_path='models/'\n",
    "# callbacks\n",
    "log_file_path = base_path + '_emotion_training.log'\n",
    "csv_logger = CSVLogger(log_file_path, append=False)\n",
    "early_stop = EarlyStopping('val_loss', patience=patience)\n",
    "reduce_lr = ReduceLROnPlateau('val_loss', factor=0.1,\n",
    "                                  patience=int(patience/4), verbose=1)\n",
    "trained_models_path = base_path + '_mini_XCEPTION'\n",
    "model_names = trained_models_path + '.{epoch:02d}-{val_acc:.2f}.hdf5'\n",
    "model_checkpoint = ModelCheckpoint(model_names, 'val_loss', verbose=1,\n",
    "                                                    save_best_only=True)\n",
    "callbacks = [model_checkpoint, csv_logger, early_stop, reduce_lr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/angara/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 28709 samples, validate on 3589 samples\n",
      "Epoch 1/60\n",
      "28709/28709 [==============================] - 591s 21ms/step - loss: 5.7815 - acc: 0.2751 - val_loss: 5.4522 - val_acc: 0.3235\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 5.45217, saving model to models/_mini_XCEPTION.01-0.32.hdf5\n",
      "Epoch 2/60\n",
      "28709/28709 [==============================] - 589s 20ms/step - loss: 5.0838 - acc: 0.3540 - val_loss: 4.7691 - val_acc: 0.3692\n",
      "\n",
      "Epoch 00002: val_loss improved from 5.45217 to 4.76907, saving model to models/_mini_XCEPTION.02-0.37.hdf5\n",
      "Epoch 3/60\n",
      "28709/28709 [==============================] - 589s 21ms/step - loss: 4.4815 - acc: 0.3912 - val_loss: 4.1879 - val_acc: 0.4057\n",
      "\n",
      "Epoch 00003: val_loss improved from 4.76907 to 4.18787, saving model to models/_mini_XCEPTION.03-0.41.hdf5\n",
      "Epoch 4/60\n",
      "28709/28709 [==============================] - 592s 21ms/step - loss: 3.9197 - acc: 0.4295 - val_loss: 3.6822 - val_acc: 0.4232\n",
      "\n",
      "Epoch 00004: val_loss improved from 4.18787 to 3.68216, saving model to models/_mini_XCEPTION.04-0.42.hdf5\n",
      "Epoch 5/60\n",
      "28709/28709 [==============================] - 589s 21ms/step - loss: 3.4251 - acc: 0.4549 - val_loss: 3.1926 - val_acc: 0.4700\n",
      "\n",
      "Epoch 00005: val_loss improved from 3.68216 to 3.19255, saving model to models/_mini_XCEPTION.05-0.47.hdf5\n",
      "Epoch 6/60\n",
      "28709/28709 [==============================] - 589s 21ms/step - loss: 2.9947 - acc: 0.4872 - val_loss: 2.9212 - val_acc: 0.4478\n",
      "\n",
      "Epoch 00006: val_loss improved from 3.19255 to 2.92119, saving model to models/_mini_XCEPTION.06-0.45.hdf5\n",
      "Epoch 7/60\n",
      "28709/28709 [==============================] - 589s 21ms/step - loss: 2.6356 - acc: 0.5166 - val_loss: 2.4823 - val_acc: 0.5127\n",
      "\n",
      "Epoch 00007: val_loss improved from 2.92119 to 2.48226, saving model to models/_mini_XCEPTION.07-0.51.hdf5\n",
      "Epoch 8/60\n",
      "28709/28709 [==============================] - 589s 21ms/step - loss: 2.3391 - acc: 0.5417 - val_loss: 2.2833 - val_acc: 0.5113\n",
      "\n",
      "Epoch 00008: val_loss improved from 2.48226 to 2.28326, saving model to models/_mini_XCEPTION.08-0.51.hdf5\n",
      "Epoch 9/60\n",
      "28709/28709 [==============================] - 589s 21ms/step - loss: 2.1008 - acc: 0.5533 - val_loss: 2.0531 - val_acc: 0.5433\n",
      "\n",
      "Epoch 00009: val_loss improved from 2.28326 to 2.05308, saving model to models/_mini_XCEPTION.09-0.54.hdf5\n",
      "Epoch 10/60\n",
      "28709/28709 [==============================] - 591s 21ms/step - loss: 1.8969 - acc: 0.5735 - val_loss: 1.9215 - val_acc: 0.5400\n",
      "\n",
      "Epoch 00010: val_loss improved from 2.05308 to 1.92147, saving model to models/_mini_XCEPTION.10-0.54.hdf5\n",
      "Epoch 11/60\n",
      "28709/28709 [==============================] - 589s 21ms/step - loss: 1.7400 - acc: 0.5889 - val_loss: 1.7763 - val_acc: 0.5556\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.92147 to 1.77628, saving model to models/_mini_XCEPTION.11-0.56.hdf5\n",
      "Epoch 12/60\n",
      "28709/28709 [==============================] - 589s 21ms/step - loss: 1.6034 - acc: 0.6059 - val_loss: 1.7205 - val_acc: 0.5486\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.77628 to 1.72048, saving model to models/_mini_XCEPTION.12-0.55.hdf5\n",
      "Epoch 13/60\n",
      "28709/28709 [==============================] - 588s 20ms/step - loss: 1.4975 - acc: 0.6145 - val_loss: 1.5877 - val_acc: 0.5701\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.72048 to 1.58768, saving model to models/_mini_XCEPTION.13-0.57.hdf5\n",
      "Epoch 14/60\n",
      "28709/28709 [==============================] - 590s 21ms/step - loss: 1.4095 - acc: 0.6266 - val_loss: 1.5389 - val_acc: 0.5645\n",
      "\n",
      "Epoch 00014: val_loss improved from 1.58768 to 1.53891, saving model to models/_mini_XCEPTION.14-0.56.hdf5\n",
      "Epoch 15/60\n",
      "28709/28709 [==============================] - 588s 20ms/step - loss: 1.3307 - acc: 0.6385 - val_loss: 1.4396 - val_acc: 0.5932\n",
      "\n",
      "Epoch 00015: val_loss improved from 1.53891 to 1.43958, saving model to models/_mini_XCEPTION.15-0.59.hdf5\n",
      "Epoch 16/60\n",
      "28709/28709 [==============================] - 588s 20ms/step - loss: 1.2656 - acc: 0.6557 - val_loss: 1.4177 - val_acc: 0.5926\n",
      "\n",
      "Epoch 00016: val_loss improved from 1.43958 to 1.41768, saving model to models/_mini_XCEPTION.16-0.59.hdf5\n",
      "Epoch 17/60\n",
      "28709/28709 [==============================] - 592s 21ms/step - loss: 1.2070 - acc: 0.6666 - val_loss: 1.4283 - val_acc: 0.5754\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 1.41768\n",
      "Epoch 18/60\n",
      "28709/28709 [==============================] - 588s 20ms/step - loss: 1.1579 - acc: 0.6785 - val_loss: 1.3692 - val_acc: 0.5929\n",
      "\n",
      "Epoch 00018: val_loss improved from 1.41768 to 1.36924, saving model to models/_mini_XCEPTION.18-0.59.hdf5\n",
      "Epoch 19/60\n",
      "28709/28709 [==============================] - 588s 20ms/step - loss: 1.1135 - acc: 0.6904 - val_loss: 1.3412 - val_acc: 0.5913\n",
      "\n",
      "Epoch 00019: val_loss improved from 1.36924 to 1.34120, saving model to models/_mini_XCEPTION.19-0.59.hdf5\n",
      "Epoch 20/60\n",
      "28709/28709 [==============================] - 588s 20ms/step - loss: 1.0726 - acc: 0.7027 - val_loss: 1.3578 - val_acc: 0.5954\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 1.34120\n",
      "Epoch 21/60\n",
      "28709/28709 [==============================] - 589s 21ms/step - loss: 1.0335 - acc: 0.7108 - val_loss: 1.3409 - val_acc: 0.5960\n",
      "\n",
      "Epoch 00021: val_loss improved from 1.34120 to 1.34090, saving model to models/_mini_XCEPTION.21-0.60.hdf5\n",
      "Epoch 22/60\n",
      "28709/28709 [==============================] - 589s 21ms/step - loss: 0.9968 - acc: 0.7212 - val_loss: 1.3523 - val_acc: 0.5963\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 1.34090\n",
      "Epoch 23/60\n",
      "28709/28709 [==============================] - 592s 21ms/step - loss: 0.9576 - acc: 0.7356 - val_loss: 1.3252 - val_acc: 0.6077\n",
      "\n",
      "Epoch 00023: val_loss improved from 1.34090 to 1.32520, saving model to models/_mini_XCEPTION.23-0.61.hdf5\n",
      "Epoch 24/60\n",
      "28709/28709 [==============================] - 589s 21ms/step - loss: 0.9372 - acc: 0.7401 - val_loss: 1.3126 - val_acc: 0.6158\n",
      "\n",
      "Epoch 00024: val_loss improved from 1.32520 to 1.31259, saving model to models/_mini_XCEPTION.24-0.62.hdf5\n",
      "Epoch 25/60\n",
      "28709/28709 [==============================] - 589s 21ms/step - loss: 0.9069 - acc: 0.7499 - val_loss: 1.3203 - val_acc: 0.6088\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 1.31259\n",
      "Epoch 26/60\n",
      "28709/28709 [==============================] - 590s 21ms/step - loss: 0.8704 - acc: 0.7646 - val_loss: 1.3155 - val_acc: 0.6121\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 1.31259\n",
      "Epoch 27/60\n",
      "28709/28709 [==============================] - 590s 21ms/step - loss: 0.8472 - acc: 0.7725 - val_loss: 1.3296 - val_acc: 0.6038\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 1.31259\n",
      "Epoch 28/60\n",
      "28709/28709 [==============================] - 589s 21ms/step - loss: 0.8227 - acc: 0.7815 - val_loss: 1.3336 - val_acc: 0.6138\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 1.31259\n",
      "Epoch 29/60\n",
      "28709/28709 [==============================] - 592s 21ms/step - loss: 0.8032 - acc: 0.7868 - val_loss: 1.3743 - val_acc: 0.6152\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 1.31259\n",
      "Epoch 30/60\n",
      "28709/28709 [==============================] - 590s 21ms/step - loss: 0.7763 - acc: 0.7975 - val_loss: 1.3861 - val_acc: 0.6055\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 1.31259\n",
      "Epoch 31/60\n",
      "28709/28709 [==============================] - 589s 21ms/step - loss: 0.7461 - acc: 0.8107 - val_loss: 1.3678 - val_acc: 0.5985\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 1.31259\n",
      "Epoch 32/60\n",
      "28709/28709 [==============================] - 590s 21ms/step - loss: 0.7294 - acc: 0.8162 - val_loss: 1.3892 - val_acc: 0.6007\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 1.31259\n",
      "Epoch 33/60\n",
      "28709/28709 [==============================] - 590s 21ms/step - loss: 0.7111 - acc: 0.8227 - val_loss: 1.4250 - val_acc: 0.6071\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 1.31259\n",
      "Epoch 34/60\n",
      "28709/28709 [==============================] - 590s 21ms/step - loss: 0.6972 - acc: 0.8286 - val_loss: 1.4130 - val_acc: 0.6121\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 1.31259\n",
      "Epoch 35/60\n",
      "28709/28709 [==============================] - 592s 21ms/step - loss: 0.6776 - acc: 0.8358 - val_loss: 1.4358 - val_acc: 0.5910\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00035: val_loss did not improve from 1.31259\n",
      "Epoch 36/60\n",
      "28709/28709 [==============================] - 586s 20ms/step - loss: 0.6541 - acc: 0.8466 - val_loss: 1.4767 - val_acc: 0.6041\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 1.31259\n",
      "\n",
      "Epoch 00036: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-06.\n",
      "Epoch 37/60\n",
      "28709/28709 [==============================] - 586s 20ms/step - loss: 0.5610 - acc: 0.8880 - val_loss: 1.3813 - val_acc: 0.6291\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 1.31259\n",
      "Epoch 38/60\n",
      "28709/28709 [==============================] - 586s 20ms/step - loss: 0.5270 - acc: 0.9014 - val_loss: 1.3779 - val_acc: 0.6333\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 1.31259\n",
      "Epoch 39/60\n",
      "28709/28709 [==============================] - 586s 20ms/step - loss: 0.5123 - acc: 0.9064 - val_loss: 1.3799 - val_acc: 0.6322\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 1.31259\n",
      "Epoch 40/60\n",
      "28709/28709 [==============================] - 586s 20ms/step - loss: 0.5012 - acc: 0.9090 - val_loss: 1.3824 - val_acc: 0.6261\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 1.31259\n",
      "Epoch 41/60\n",
      "28709/28709 [==============================] - 589s 21ms/step - loss: 0.4864 - acc: 0.9162 - val_loss: 1.3836 - val_acc: 0.6261\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 1.31259\n",
      "Epoch 42/60\n",
      "28709/28709 [==============================] - 586s 20ms/step - loss: 0.4762 - acc: 0.9181 - val_loss: 1.3856 - val_acc: 0.6300\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 1.31259\n",
      "Epoch 43/60\n",
      "28709/28709 [==============================] - 587s 20ms/step - loss: 0.4659 - acc: 0.9221 - val_loss: 1.3837 - val_acc: 0.6241\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 1.31259\n",
      "Epoch 44/60\n",
      "28709/28709 [==============================] - 586s 20ms/step - loss: 0.4620 - acc: 0.9238 - val_loss: 1.3813 - val_acc: 0.6283\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 1.31259\n",
      "Epoch 45/60\n",
      "28709/28709 [==============================] - 586s 20ms/step - loss: 0.4515 - acc: 0.9235 - val_loss: 1.3888 - val_acc: 0.6308\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 1.31259\n",
      "Epoch 46/60\n",
      "28709/28709 [==============================] - 586s 20ms/step - loss: 0.4423 - acc: 0.9269 - val_loss: 1.3864 - val_acc: 0.6283\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 1.31259\n",
      "Epoch 47/60\n",
      "28709/28709 [==============================] - 589s 21ms/step - loss: 0.4376 - acc: 0.9294 - val_loss: 1.3916 - val_acc: 0.6283\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 1.31259\n",
      "Epoch 48/60\n",
      "28709/28709 [==============================] - 586s 20ms/step - loss: 0.4294 - acc: 0.9313 - val_loss: 1.3920 - val_acc: 0.6247\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 1.31259\n",
      "\n",
      "Epoch 00048: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-07.\n",
      "Epoch 49/60\n",
      "28709/28709 [==============================] - 587s 20ms/step - loss: 0.4138 - acc: 0.9383 - val_loss: 1.3911 - val_acc: 0.6247\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 1.31259\n",
      "Epoch 50/60\n",
      "28709/28709 [==============================] - 587s 20ms/step - loss: 0.4145 - acc: 0.9370 - val_loss: 1.3903 - val_acc: 0.6272\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 1.31259\n",
      "Epoch 51/60\n",
      "28709/28709 [==============================] - 587s 20ms/step - loss: 0.4162 - acc: 0.9365 - val_loss: 1.3906 - val_acc: 0.6266\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 1.31259\n",
      "Epoch 52/60\n",
      "28709/28709 [==============================] - 587s 20ms/step - loss: 0.4174 - acc: 0.9358 - val_loss: 1.3915 - val_acc: 0.6252\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 1.31259\n",
      "Epoch 53/60\n",
      "28709/28709 [==============================] - 590s 21ms/step - loss: 0.4109 - acc: 0.9383 - val_loss: 1.3912 - val_acc: 0.6258\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 1.31259\n",
      "Epoch 54/60\n",
      "28709/28709 [==============================] - 587s 20ms/step - loss: 0.4149 - acc: 0.9357 - val_loss: 1.3925 - val_acc: 0.6261\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 1.31259\n",
      "Epoch 55/60\n",
      "28709/28709 [==============================] - 588s 20ms/step - loss: 0.4127 - acc: 0.9371 - val_loss: 1.3928 - val_acc: 0.6258\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 1.31259\n",
      "Epoch 56/60\n",
      "28709/28709 [==============================] - 587s 20ms/step - loss: 0.4130 - acc: 0.9372 - val_loss: 1.3918 - val_acc: 0.6255\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 1.31259\n",
      "Epoch 57/60\n",
      "28709/28709 [==============================] - 587s 20ms/step - loss: 0.4093 - acc: 0.9390 - val_loss: 1.3922 - val_acc: 0.6258\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 1.31259\n",
      "Epoch 58/60\n",
      "28709/28709 [==============================] - 586s 20ms/step - loss: 0.4089 - acc: 0.9387 - val_loss: 1.3932 - val_acc: 0.6250\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 1.31259\n",
      "Epoch 59/60\n",
      "28709/28709 [==============================] - 589s 21ms/step - loss: 0.4063 - acc: 0.9397 - val_loss: 1.3931 - val_acc: 0.6280\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 1.31259\n",
      "Epoch 60/60\n",
      "28709/28709 [==============================] - 587s 20ms/step - loss: 0.4074 - acc: 0.9382 - val_loss: 1.3931 - val_acc: 0.6289\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 1.31259\n",
      "\n",
      "Epoch 00060: ReduceLROnPlateau reducing learning rate to 9.999999974752428e-08.\n",
      "saving whole model\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Training the model\n",
    "model.fit(x_train,y_train,verbose=1,batch_size=64,epochs=60,callbacks=callbacks,\n",
    "          validation_data=(x_test,y_test))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#print('saving whole model')\n",
    "#Saving the  model to  use it later on\n",
    "#fer_json = model.to_json()\n",
    "#with open(\"model.json\", \"w\") as json_file:\n",
    " #   json_file.write(fer_json)\n",
    "#model.save_weights(\"weights.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
